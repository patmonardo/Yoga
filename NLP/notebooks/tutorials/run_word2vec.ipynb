{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Word2Vec Model\n",
        "==============\n",
        "\n",
        "Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you missed the buzz, Word2Vec is a widely used algorithm based on neural\n",
        "networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n",
        "Using large amounts of unannotated plain text, word2vec learns relationships\n",
        "between words automatically. The output are vectors, one vector per word,\n",
        "with remarkable linear relationships that allow us to do things like:\n",
        "\n",
        "* vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n",
        "* vec(\"Montreal Canadiens\") – vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\").\n",
        "\n",
        "Word2vec is very useful in `automatic text tagging\n",
        "<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\\ , recommender\n",
        "systems and machine translation.\n",
        "\n",
        "This tutorial:\n",
        "\n",
        "#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n",
        "#. Shows off a demo of ``Word2Vec`` using a pre-trained model\n",
        "#. Demonstrates training a new model from your own data\n",
        "#. Demonstrates loading and saving models\n",
        "#. Introduces several training parameters and demonstrates their effect\n",
        "#. Discusses memory requirements\n",
        "#. Visualizes Word2Vec embeddings by applying dimensionality reduction\n",
        "\n",
        "Review: Bag-of-words\n",
        "--------------------\n",
        "\n",
        ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
        "\n",
        "You may be familiar with the `bag-of-words model\n",
        "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
        "`core_concepts_vector` section.\n",
        "This model transforms each document to a fixed-length vector of integers.\n",
        "For example, given the sentences:\n",
        "\n",
        "- ``John likes to watch movies. Mary likes movies too.``\n",
        "- ``John also likes to watch football games. Mary hates football.``\n",
        "\n",
        "The model outputs the vectors:\n",
        "\n",
        "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
        "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
        "\n",
        "Each vector has 10 elements, where each element counts the number of times a\n",
        "particular word occurred in the document.\n",
        "The order of elements is arbitrary.\n",
        "In the example above, the order of the elements corresponds to the words:\n",
        "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
        "\n",
        "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
        "\n",
        "First, they lose all information about word order: \"John likes Mary\" and\n",
        "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
        "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
        "models consider word phrases of length n to represent documents as\n",
        "fixed-length vectors to capture local word order but suffer from data\n",
        "sparsity and high dimensionality.\n",
        "\n",
        "Second, the model does not attempt to learn the meaning of the underlying\n",
        "words, and as a consequence, the distance between vectors doesn't always\n",
        "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
        "second problem.\n",
        "\n",
        "Introducing: the ``Word2Vec`` Model\n",
        "-----------------------------------\n",
        "\n",
        "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
        "vector space using a shallow neural network. The result is a set of\n",
        "word-vectors where vectors close together in vector space have similar\n",
        "meanings based on context, and word-vectors distant to each other have\n",
        "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
        "together and ``strong`` and ``Paris`` would be relatively far.\n",
        "\n",
        "The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\n",
        "class implements them both:\n",
        "\n",
        "1. Skip-grams (SG)\n",
        "2. Continuous-bag-of-words (CBOW)\n",
        "\n",
        ".. Important::\n",
        "  Don't let the implementation details below scare you.\n",
        "  They're advanced material: if it's too much, then move on to the next section.\n",
        "\n",
        "The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__\n",
        "model, for example, takes in pairs (word1, word2) generated by moving a\n",
        "window across text data, and trains a 1-hidden-layer neural network based on\n",
        "the synthetic task of given an input word, giving us a predicted probability\n",
        "distribution of nearby words to the input. A virtual `one-hot\n",
        "<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words\n",
        "goes through a 'projection layer' to the hidden layer; these projection\n",
        "weights are later interpreted as the word embeddings. So if the hidden layer\n",
        "has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
        "\n",
        "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It\n",
        "is also a 1-hidden-layer neural network. The synthetic training task now uses\n",
        "the average of multiple input context words, rather than a single word as in\n",
        "skip-gram, to predict the center word. Again, the projection weights that\n",
        "turn one-hot words into averageable vectors, of the same width as the hidden\n",
        "layer, are interpreted as the word embeddings.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec Demo\n",
        "-------------\n",
        "\n",
        "To see what ``Word2Vec`` can do, let's download a pre-trained model and play\n",
        "around with it. We will fetch the Word2Vec model trained on part of the\n",
        "Google News dataset, covering approximately 3 million words and phrases. Such\n",
        "a model can take hours to train, but since it's already available,\n",
        "downloading and loading it with Gensim takes minutes.\n",
        "\n",
        ".. Important::\n",
        "  The model is approximately 2GB, so you'll need a decent network connection\n",
        "  to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n",
        "  below.\n",
        "\n",
        "You may also check out an `online word2vec demo\n",
        "<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try\n",
        "this vector algebra for yourself. That demo runs ``word2vec`` on the\n",
        "**entire** Google News dataset, of **about 100 billion words**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A common operation is to retrieve the vocabulary of a model. That is trivial:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily obtain vectors for terms the model is familiar with:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vec_king = wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, the model is unable to infer vectors for unfamiliar words.\n",
        "This is one limitation of Word2Vec: if this limitation matters to you, check\n",
        "out the FastText model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    vec_cameroon = wv['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving on, ``Word2Vec`` supports several word similarity tasks out of the\n",
        "box.  You can see how the similarity intuitively decreases as the words get\n",
        "less and less similar.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the 5 most similar words to \"car\" or \"minivan\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which of the below does not belong in the sequence?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Your Own Model\n",
        "-----------------------\n",
        "\n",
        "To start, you'll need some data for training the model. For the following\n",
        "examples, we'll use the `Lee Evaluation Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "(which you `already have\n",
        "<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_\n",
        "if you've installed Gensim).\n",
        "\n",
        "This corpus is small enough to fit entirely in memory, but we'll implement a\n",
        "memory-friendly iterator that reads it line-by-line to demonstrate how you\n",
        "would handle a larger corpus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = \\\n",
        "            '/home/pat/Documents/Yoga/Hegel/dharma-greater-bhasya.tex' \n",
        "        #datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wanted to do any custom preprocessing, e.g. decode a non-standard\n",
        "encoding, lowercase, remove numbers, extract named entities... All of this can\n",
        "be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to\n",
        "know. All that is required is that the input yields one sentence (list of\n",
        "utf8 words) after another.\n",
        "\n",
        "Let's go ahead and train a model on our corpus.  Don't worry about the\n",
        "training parameters much for now, we'll revisit them later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 12:06:41,138 : INFO : collecting all words and their counts\n",
            "2023-09-28 12:06:41,143 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2023-09-28 12:06:41,493 : INFO : collected 1996 word types from a corpus of 56714 raw words and 8895 sentences\n",
            "2023-09-28 12:06:41,495 : INFO : Creating a fresh vocabulary\n",
            "2023-09-28 12:06:41,506 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 617 unique words (30.91% of original 1996, drops 1379)', 'datetime': '2023-09-28T12:06:41.506823', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 12:06:41,509 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 54568 word corpus (96.22% of original 56714, drops 2146)', 'datetime': '2023-09-28T12:06:41.509413', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 12:06:41,529 : INFO : deleting the raw counts dictionary of 1996 items\n",
            "2023-09-28 12:06:41,532 : INFO : sample=0.001 downsamples 71 most-common words\n",
            "2023-09-28 12:06:41,535 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 29895.057621750308 word corpus (54.8%% of prior 54568)', 'datetime': '2023-09-28T12:06:41.535284', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 12:06:41,575 : INFO : estimated required memory for 617 words and 100 dimensions: 802100 bytes\n",
            "2023-09-28 12:06:41,577 : INFO : resetting layer weights\n",
            "2023-09-28 12:06:41,590 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-28T12:06:41.590527', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'build_vocab'}\n",
            "2023-09-28 12:06:41,593 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 617 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-28T12:06:41.593369', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'train'}\n",
            "2023-09-28 12:06:42,240 : INFO : EPOCH 0: training on 56714 raw words (29750 effective words) took 0.6s, 47370 effective words/s\n",
            "2023-09-28 12:06:42,689 : INFO : EPOCH 1: training on 56714 raw words (29945 effective words) took 0.4s, 67712 effective words/s\n",
            "2023-09-28 12:06:43,129 : INFO : EPOCH 2: training on 56714 raw words (29986 effective words) took 0.4s, 70311 effective words/s\n",
            "2023-09-28 12:06:43,536 : INFO : EPOCH 3: training on 56714 raw words (29976 effective words) took 0.4s, 76608 effective words/s\n",
            "2023-09-28 12:06:43,929 : INFO : EPOCH 4: training on 56714 raw words (29880 effective words) took 0.4s, 76949 effective words/s\n",
            "2023-09-28 12:06:43,931 : INFO : Word2Vec lifecycle event {'msg': 'training on 283570 raw words (149537 effective words) took 2.3s, 64025 effective words/s', 'datetime': '2023-09-28T12:06:43.931480', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'train'}\n",
            "2023-09-28 12:06:43,933 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=617, vector_size=100, alpha=0.025>', 'datetime': '2023-09-28T12:06:43.933315', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have our model, we can use it in the same way as in the demo above.\n",
        "\n",
        "The main part of the model is ``model.wv``\\ , where \"wv\" stands for \"word vectors\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.05004038,  0.09318157, -0.08379216,  0.12509848,  0.10288828,\n",
              "       -0.34134796,  0.2325755 ,  0.40508127, -0.31315938, -0.16669121,\n",
              "       -0.05746438, -0.28902462, -0.08568899,  0.17947996,  0.13844413,\n",
              "        0.15177518,  0.18965374,  0.03308851, -0.16135505, -0.6244506 ,\n",
              "        0.13323377,  0.04702511,  0.27903202, -0.04215534, -0.06947714,\n",
              "        0.07411466, -0.34302258,  0.02207917, -0.19585937,  0.03944722,\n",
              "        0.21331765,  0.14738125,  0.20491776, -0.17953135, -0.25803503,\n",
              "        0.42124385,  0.08357263, -0.07592358, -0.23335049, -0.07370488,\n",
              "        0.19670002, -0.1775679 , -0.30449522, -0.05849155,  0.38585526,\n",
              "       -0.01102275,  0.06963586, -0.20733687,  0.29782385,  0.08904297,\n",
              "        0.16242419, -0.12006561, -0.10303345, -0.04158401, -0.16364773,\n",
              "        0.22186723, -0.055732  , -0.14397372, -0.19483419, -0.01104603,\n",
              "        0.07929608, -0.16798505,  0.13588963, -0.0179831 , -0.15580064,\n",
              "        0.28097346,  0.08828127,  0.23718986, -0.30730864,  0.40252677,\n",
              "       -0.19524664,  0.00964252,  0.19181482,  0.07008314,  0.08305071,\n",
              "        0.1335637 , -0.0111692 , -0.02624544, -0.08296224,  0.12448349,\n",
              "       -0.13119851,  0.02259942, -0.22536962,  0.40437207, -0.1309184 ,\n",
              "       -0.05886513,  0.1378893 ,  0.19907126,  0.27350217,  0.05038448,\n",
              "        0.42886415,  0.16245411,  0.00802427, -0.0779454 ,  0.30603495,\n",
              "        0.10840791,  0.14711076, -0.18850558, -0.03869259,  0.04258774],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_king = model.wv #['king']\n",
        "vec_king[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieving the vocabulary works the same way:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/617 is the\n",
            "word #1/617 is is\n",
            "word #2/617 is of\n",
            "word #3/617 is in\n",
            "word #4/617 is as\n",
            "word #5/617 is and\n",
            "word #6/617 is it\n",
            "word #7/617 is itself\n",
            "word #8/617 is this\n",
            "word #9/617 is to\n",
            "word #10/617 is its\n",
            "word #11/617 is that\n",
            "word #12/617 is but\n",
            "word #13/617 is which\n",
            "word #14/617 is other\n",
            "word #15/617 is only\n",
            "word #16/617 is or\n",
            "word #17/617 is not\n",
            "word #18/617 is reflection\n",
            "word #19/617 is self\n",
            "word #20/617 is into\n",
            "word #21/617 is ground\n",
            "word #22/617 is being\n",
            "word #23/617 is has\n",
            "word #24/617 is an\n",
            "word #25/617 is form\n",
            "word #26/617 is essence\n",
            "word #27/617 is are\n",
            "word #28/617 is for\n",
            "word #29/617 is content\n",
            "word #30/617 is identity\n",
            "word #31/617 is therefore\n",
            "word #32/617 is positedness\n",
            "word #33/617 is one\n",
            "word #34/617 is absolute\n",
            "word #35/617 is negative\n",
            "word #36/617 is existence\n",
            "word #37/617 is immediate\n",
            "word #38/617 is from\n",
            "word #39/617 is unity\n",
            "word #40/617 is first\n",
            "word #41/617 is posited\n",
            "word #42/617 is at\n",
            "word #43/617 is determination\n",
            "word #44/617 is with\n",
            "word #45/617 is each\n",
            "word #46/617 is their\n",
            "word #47/617 is concrete\n",
            "word #48/617 is thus\n",
            "word #49/617 is external\n",
            "word #50/617 is same\n",
            "word #51/617 is determinateness\n",
            "word #52/617 is subsistence\n",
            "word #53/617 is something\n",
            "word #54/617 is determined\n",
            "word #55/617 is also\n",
            "word #56/617 is be\n",
            "word #57/617 is because\n",
            "word #58/617 is immediacy\n",
            "word #59/617 is thing\n",
            "word #60/617 is reference\n",
            "word #61/617 is determinations\n",
            "word #62/617 is two\n",
            "word #63/617 is such\n",
            "word #64/617 is by\n",
            "word #65/617 is reflected\n",
            "word #66/617 is so\n",
            "word #67/617 is on\n",
            "word #68/617 is they\n",
            "word #69/617 is sublated\n",
            "word #70/617 is through\n",
            "word #71/617 is have\n",
            "word #72/617 is appearance\n",
            "word #73/617 is difference\n",
            "word #74/617 is negation\n",
            "word #75/617 is cause\n",
            "word #76/617 is actuality\n",
            "word #77/617 is what\n",
            "word #78/617 is essential\n",
            "word #79/617 is possibility\n",
            "word #80/617 is connection\n",
            "word #81/617 is matter\n",
            "word #82/617 is time\n",
            "word #83/617 is relation\n",
            "word #84/617 is just\n",
            "word #85/617 is identical\n",
            "word #86/617 is equally\n",
            "word #87/617 is indifferent\n",
            "word #88/617 is effect\n",
            "word #89/617 is whole\n",
            "word #90/617 is over\n",
            "word #91/617 is within\n",
            "word #92/617 is shine\n",
            "word #93/617 is moment\n",
            "word #94/617 is now\n",
            "word #95/617 is movement\n",
            "word #96/617 is simple\n",
            "word #97/617 is own\n",
            "word #98/617 is substance\n",
            "word #99/617 is positive\n"
          ]
        }
      ],
      "source": [
        "wv = model.wv\n",
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 1000:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing and loading models\n",
        "--------------------------\n",
        "\n",
        "You'll notice that training non-trivial models can take time.  Once you've\n",
        "trained your model and it works as expected, you can save it to disk.  That\n",
        "way, you don't have to spend time training it all over again later.\n",
        "\n",
        "You can store/load models using the standard gensim methods:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 12:11:42,141 : INFO : Word2Vec lifecycle event {'fname_or_handle': '/tmp/gensim-model-2guznnd6', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-09-28T12:11:42.141841', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'saving'}\n",
            "2023-09-28 12:11:42,149 : INFO : not storing attribute cum_table\n",
            "2023-09-28 12:11:42,158 : INFO : saved /tmp/gensim-model-2guznnd6\n",
            "2023-09-28 12:11:42,161 : INFO : loading Word2Vec object from /tmp/gensim-model-2guznnd6\n",
            "2023-09-28 12:11:42,167 : INFO : loading wv recursively from /tmp/gensim-model-2guznnd6.wv.* with mmap=None\n",
            "2023-09-28 12:11:42,170 : INFO : setting ignored attribute cum_table to None\n",
            "2023-09-28 12:11:42,209 : INFO : Word2Vec lifecycle event {'fname': '/tmp/gensim-model-2guznnd6', 'datetime': '2023-09-28T12:11:42.209432', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'loaded'}\n"
          ]
        }
      ],
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    #\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc.\n",
        "    #\n",
        "    # To load a saved model:\n",
        "    #\n",
        "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "which uses pickle internally, optionally ``mmap``\\ ‘ing the model’s internal\n",
        "large NumPy matrices into virtual memory directly from disk files, for\n",
        "inter-process memory sharing.\n",
        "\n",
        "In addition, you can load models created by the original C tool, both using\n",
        "its text and binary formats::\n",
        "\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
        "  # using gzipped/bz2 input works too, no need to unzip\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Parameters\n",
        "-------------------\n",
        "\n",
        "``Word2Vec`` accepts several parameters that affect both training speed and quality.\n",
        "\n",
        "min_count\n",
        "---------\n",
        "\n",
        "``min_count`` is for pruning the internal dictionary. Words that appear only\n",
        "once or twice in a billion-word corpus are probably uninteresting typos and\n",
        "garbage. In addition, there’s not enough data to make any meaningful training\n",
        "on those words, so it’s best to ignore them:\n",
        "\n",
        "default value of min_count=5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec(sentences, min_count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "vector_size\n",
        "-----------\n",
        "\n",
        "``vector_size`` is the number of dimensions (N) of the N-dimensional space that\n",
        "gensim Word2Vec maps the words onto.\n",
        "\n",
        "Bigger size values require more training data, but can lead to better (more\n",
        "accurate) models. Reasonable values are in the tens to hundreds.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The default value of vector_size is 100.\n",
        "model = gensim.models.Word2Vec(sentences, vector_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "workers\n",
        "-------\n",
        "\n",
        "``workers`` , the last of the major parameters (full list `here\n",
        "<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)\n",
        "is for training parallelization, to speed up training:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# default value of workers=3 (tutorial says 1...)\n",
        "model = gensim.models.Word2Vec(sentences, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``workers`` parameter only has an effect if you have `Cython\n",
        "<http://cython.org/>`_ installed. Without Cython, you’ll only be able to use\n",
        "one core because of the `GIL\n",
        "<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``\n",
        "training will be `miserably slow\n",
        "<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\\ ).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory\n",
        "------\n",
        "\n",
        "At its core, ``word2vec`` model parameters are stored as matrices (NumPy\n",
        "arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)\n",
        "times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).\n",
        "\n",
        "Three such matrices are held in RAM (work is underway to reduce that number\n",
        "to two, or even one). So if your input contains 100,000 unique words, and you\n",
        "asked for layer ``vector_size=200``\\ , the model will require approx.\n",
        "``100,000*200*4*3 bytes = ~229MB``.\n",
        "\n",
        "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would\n",
        "take a few megabytes), but unless your words are extremely loooong strings, memory\n",
        "footprint will be dominated by the three matrices above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating\n",
        "----------\n",
        "\n",
        "``Word2Vec`` training is an unsupervised task, there’s no good way to\n",
        "objectively evaluate the result. Evaluation depends on your end application.\n",
        "\n",
        "Google has released their testing set of about 20,000 syntactic and semantic\n",
        "test examples, following the “A is to B as C is to D” task. It is provided in\n",
        "the 'datasets' folder.\n",
        "\n",
        "For example a syntactic analogy of comparative type is ``bad:worse;good:?``.\n",
        "There are total of 9 types of syntactic comparisons in the dataset like\n",
        "plural nouns and nouns of opposite meaning.\n",
        "\n",
        "The semantic questions contain five types of semantic analogies, such as\n",
        "capital cities (``Paris:France;Tokyo:?``) or family members\n",
        "(``brother:sister;dad:?``).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gensim supports the same evaluation set, in exactly the same format:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``evaluate_word_analogies`` method takes an `optional parameter\n",
        "<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_\n",
        "``restrict_vocab`` which limits which test examples are to be considered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
        "\n",
        "By default it uses an academic dataset WS-353 but one can create a dataset\n",
        "specific to your business based on it. It contains word pairs together with\n",
        "human-assigned similarity judgments. It measures the relatedness or\n",
        "co-occurrence of two words. For example, 'coast' and 'shore' are very similar\n",
        "as they appear in the same context. At the same time 'clothes' and 'closet'\n",
        "are less similar because they are related but not interchangeable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Important::\n",
        "  Good performance on Google's or WS-353 test set doesn’t mean word2vec will\n",
        "  work well in your application, or vice versa. It’s always best to evaluate\n",
        "  directly on your intended task. For an example of how to use word2vec in a\n",
        "  classifier pipeline, see this `tutorial\n",
        "  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Online training / Resuming training\n",
        "-----------------------------------\n",
        "\n",
        "Advanced users can load a model and continue training it with more sentences\n",
        "and `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
        "more_sentences = [\n",
        "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
        "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
        "]\n",
        "model.build_vocab(more_sentences, update=True)\n",
        "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# cleaning up temporary file\n",
        "import os\n",
        "os.remove(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may need to tweak the ``total_words`` parameter to ``train()``,\n",
        "depending on what learning rate decay you want to simulate.\n",
        "\n",
        "Note that it’s not possible to resume training with models generated by the C\n",
        "tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\n",
        "querying/similarity, but information vital for training (the vocab tree) is\n",
        "missing there.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loss Computation\n",
        "-------------------------\n",
        "\n",
        "The parameter ``compute_loss`` can be used to toggle computation of loss\n",
        "while training the Word2Vec model. The computed loss is stored in the model\n",
        "attribute ``running_training_loss`` and can be retrieved using the function\n",
        "``get_latest_training_loss`` as follows :\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# instantiating and training the Word2Vec model\n",
        "model_with_loss = gensim.models.Word2Vec(\n",
        "    sentences,\n",
        "    min_count=1,\n",
        "    compute_loss=True,\n",
        "    hs=0,\n",
        "    sg=1,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# getting the training loss value\n",
        "training_loss = model_with_loss.get_latest_training_loss()\n",
        "print(training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks\n",
        "----------\n",
        "\n",
        "Let's run some benchmarks to see effect of the training loss computation code\n",
        "on training time.\n",
        "\n",
        "We'll use the following data for the benchmarks:\n",
        "\n",
        "#. Lee Background corpus: included in gensim's test data\n",
        "#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the\n",
        "   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "import gensim.models.word2vec\n",
        "import gensim.downloader as api\n",
        "import smart_open\n",
        "\n",
        "\n",
        "def head(path, size):\n",
        "    with smart_open.open(path) as fin:\n",
        "        return io.StringIO(fin.read(size))\n",
        "\n",
        "\n",
        "def generate_input_data():\n",
        "    lee_path = datapath('lee_background.cor')\n",
        "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
        "    ls.name = '25kB'\n",
        "    yield ls\n",
        "\n",
        "    text8_path = api.load('text8').fn\n",
        "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
        "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
        "    for l, s in zip(labels, sizes):\n",
        "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
        "        ls.name = l\n",
        "        yield ls\n",
        "\n",
        "\n",
        "input_data = list(generate_input_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compare the training time taken for different combinations of input\n",
        "data and model training parameters like ``hs`` and ``sg``.\n",
        "\n",
        "For each combination, we repeat the test several times to obtain the mean and\n",
        "standard deviation of the test duration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Temporarily reduce logging verbosity\n",
        "logging.root.level = logging.ERROR\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_time_values = []\n",
        "seed_val = 42\n",
        "sg_values = [0, 1]\n",
        "hs_values = [0, 1]\n",
        "\n",
        "fast = True\n",
        "if fast:\n",
        "    input_data_subset = input_data[:3]\n",
        "else:\n",
        "    input_data_subset = input_data\n",
        "\n",
        "\n",
        "for data in input_data_subset:\n",
        "    for sg_val in sg_values:\n",
        "        for hs_val in hs_values:\n",
        "            for loss_flag in [True, False]:\n",
        "                time_taken_list = []\n",
        "                for i in range(3):\n",
        "                    start_time = time.time()\n",
        "                    w2v_model = gensim.models.Word2Vec(\n",
        "                        data,\n",
        "                        compute_loss=loss_flag,\n",
        "                        sg=sg_val,\n",
        "                        hs=hs_val,\n",
        "                        seed=seed_val,\n",
        "                    )\n",
        "                    time_taken_list.append(time.time() - start_time)\n",
        "\n",
        "                time_taken_list = np.array(time_taken_list)\n",
        "                time_mean = np.mean(time_taken_list)\n",
        "                time_std = np.std(time_taken_list)\n",
        "\n",
        "                model_result = {\n",
        "                    'train_data': data.name,\n",
        "                    'compute_loss': loss_flag,\n",
        "                    'sg': sg_val,\n",
        "                    'hs': hs_val,\n",
        "                    'train_time_mean': time_mean,\n",
        "                    'train_time_std': time_std,\n",
        "                }\n",
        "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
        "                train_time_values.append(model_result)\n",
        "\n",
        "train_times_table = pd.DataFrame(train_time_values)\n",
        "train_times_table = train_times_table.sort_values(\n",
        "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
        "    ascending=[False, False, True, False],\n",
        ")\n",
        "print(train_times_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualising Word Embeddings\n",
        "---------------------------\n",
        "\n",
        "The word embeddings made by the model can be visualised by reducing\n",
        "dimensionality of the words to 2 dimensions using tSNE.\n",
        "\n",
        "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n",
        "* Syntactic: words like run, running or cut, cutting lie close together.\n",
        "\n",
        "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
        "\n",
        ".. Important::\n",
        "  The model used for the visualisation is trained on a small corpus. Thus\n",
        "  some of the relations might not be so clear.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "\n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    # extract the words & their vectors, as numpy arrays\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)\n",
        "\n",
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "\n",
        "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "\n",
        "    #\n",
        "    # Label randomly subsampled 25 data points\n",
        "    #\n",
        "    indices = list(range(len(labels)))\n",
        "    selected_indices = random.sample(indices, 25)\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
        "\n",
        "try:\n",
        "    get_ipython()\n",
        "except Exception:\n",
        "    plot_function = plot_with_matplotlib\n",
        "else:\n",
        "    plot_function = plot_with_plotly\n",
        "\n",
        "plot_function(x_vals, y_vals, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n",
        "----------\n",
        "\n",
        "In this tutorial we learned how to train word2vec models on your custom data\n",
        "and also how to evaluate it. Hope that you too will find this popular tool\n",
        "useful in your Machine Learning tasks!\n",
        "\n",
        "Links\n",
        "-----\n",
        "\n",
        "- API docs: :py:mod:`gensim.models.word2vec`\n",
        "- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
