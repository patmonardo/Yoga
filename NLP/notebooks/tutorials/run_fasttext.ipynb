{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# FastText Model\n",
        "\n",
        "Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we'll learn to work with fastText library for training word-embedding\n",
        "models, saving & loading them and performing similarity operations & vector\n",
        "lookups analogous to Word2Vec.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to use fastText?\n",
        "\n",
        "The main principle behind [fastText](https://github.com/facebookresearch/fastText) is that the\n",
        "morphological structure of a word carries important information about the meaning of the word.\n",
        "Such structure is not taken into account by traditional word embeddings like Word2Vec, which\n",
        "train a unique word embedding for every individual word.\n",
        "This is especially significant for morphologically rich languages (German, Turkish) in which a\n",
        "single word can have a large number of morphological forms, each of which might occur rarely,\n",
        "thus making it hard to train good word embeddings.\n",
        "\n",
        "\n",
        "fastText attempts to solve this by treating each word as the aggregation of its subwords.\n",
        "For the sake of simplicity and language-independence, subwords are taken to be the character ngrams\n",
        "of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
        "\n",
        "\n",
        "According to a detailed comparison of Word2Vec and fastText in\n",
        "[this notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)_,\n",
        "fastText does significantly better on syntactic tasks as compared to the original Word2Vec,\n",
        "especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText\n",
        "on semantic tasks though. The differences grow smaller as the size of the training corpus increases.\n",
        "\n",
        "\n",
        "fastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its\n",
        "component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the following examples, we'll use the Lee Corpus (which you already have if you've installed Gensim) for training our model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 11:37:22,946 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
            "2023-09-28 11:37:22,948 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
            "2023-09-28 11:37:22,954 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2023-09-28T11:37:22.954831', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'created'}\n",
            "2023-09-28 11:37:23,168 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2023-09-28T11:37:23.168619', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'created'}\n",
            "2023-09-28 11:37:23,170 : INFO : collecting all words and their counts\n",
            "2023-09-28 11:37:23,172 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2023-09-28 11:37:23,212 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2023-09-28 11:37:23,213 : INFO : Creating a fresh vocabulary\n",
            "2023-09-28 11:37:23,240 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2023-09-28T11:37:23.240318', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 11:37:23,241 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2023-09-28T11:37:23.241807', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 11:37:23,297 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2023-09-28 11:37:23,302 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2023-09-28 11:37:23,305 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2023-09-28T11:37:23.305138', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'prepare_vocab'}\n",
            "2023-09-28 11:37:23,450 : INFO : estimated required memory for 1762 words, 2000000 buckets and 100 dimensions: 802597824 bytes\n",
            "2023-09-28 11:37:23,453 : INFO : resetting layer weights\n",
            "2023-09-28 11:37:28,743 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-09-28T11:37:28.743383', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'build_vocab'}\n",
            "2023-09-28 11:37:28,745 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-09-28T11:37:28.745773', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'train'}\n",
            "2023-09-28 11:37:29,195 : INFO : EPOCH 0: training on 60387 raw words (32958 effective words) took 0.4s, 81126 effective words/s\n",
            "2023-09-28 11:37:29,678 : INFO : EPOCH 1: training on 60387 raw words (32906 effective words) took 0.5s, 71058 effective words/s\n",
            "2023-09-28 11:37:30,276 : INFO : EPOCH 2: training on 60387 raw words (32863 effective words) took 0.6s, 58472 effective words/s\n",
            "2023-09-28 11:37:30,690 : INFO : EPOCH 3: training on 60387 raw words (32832 effective words) took 0.4s, 86101 effective words/s\n",
            "2023-09-28 11:37:31,046 : INFO : EPOCH 4: training on 60387 raw words (32827 effective words) took 0.3s, 99548 effective words/s\n",
            "2023-09-28 11:37:31,048 : INFO : FastText lifecycle event {'msg': 'training on 301935 raw words (164386 effective words) took 2.3s, 71495 effective words/s', 'datetime': '2023-09-28T11:37:31.048790', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<gensim.models.fasttext.FastText object at 0x7fac98cc7110>\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint as print\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "# Set file names for train and test data\n",
        "corpus_file = datapath('lee_background.cor')\n",
        "\n",
        "model = FastText(vector_size=100)\n",
        "\n",
        "# build the vocabulary\n",
        "model.build_vocab(corpus_file=corpus_file)\n",
        "\n",
        "# train the model\n",
        "model.train(\n",
        "    corpus_file=corpus_file, epochs=model.epochs,\n",
        "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training hyperparameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n",
        "\n",
        "- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
        "- vector_size: Dimensionality of vector embeddings to be learnt (Default 100)\n",
        "- alpha: Initial learning rate (Default 0.025)\n",
        "- window: Context window size (Default 5)\n",
        "- min_count: Ignore words with number of occurrences below this (Default 5)\n",
        "- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
        "- sample: Threshold for downsampling higher-frequency words (Default 0.001)\n",
        "- negative: Number of negative words to sample, for `ns` (Default 5)\n",
        "- epochs: Number of epochs (Default 5)\n",
        "- sorted_vocab: Sort vocab by descending frequency (Default 1)\n",
        "- threads: Number of threads to use (Default 12)\n",
        "\n",
        "\n",
        "In addition, fastText has three additional parameters:\n",
        "\n",
        "- min_n: min length of char ngrams (Default 3)\n",
        "- max_n: max length of char ngrams (Default 6)\n",
        "- bucket: number of buckets used for hashing ngrams (Default 2000000)\n",
        "\n",
        "\n",
        "Parameters ``min_n`` and ``max_n`` control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0, or to be lesser than ``min_n``\\ , no character ngrams are used, and the model effectively reduces to Word2Vec.\n",
        "\n",
        "\n",
        "\n",
        "To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the [Fowler-Noll-Vo hashing function](http://www.isthe.com/chongo/tech/comp/fnv) (FNV-1a variant) is employed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** You can continue to train your model while using Gensim's native implementation of fastText.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving/loading models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Models can be saved and loaded via the ``load`` and ``save`` methods, just like\n",
        "any other model in Gensim.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 11:37:31,304 : INFO : FastText lifecycle event {'fname_or_handle': '/tmp/saved_model_gensim-gx2m_wjw', 'separately': '[]', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-09-28T11:37:31.304884', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'saving'}\n",
            "2023-09-28 11:37:31,307 : INFO : storing np array 'vectors_ngrams' to /tmp/saved_model_gensim-gx2m_wjw.wv.vectors_ngrams.npy\n",
            "2023-09-28 11:37:38,060 : INFO : not storing attribute buckets_word\n",
            "2023-09-28 11:37:38,062 : INFO : not storing attribute vectors\n",
            "2023-09-28 11:37:38,065 : INFO : not storing attribute cum_table\n",
            "2023-09-28 11:37:38,106 : INFO : saved /tmp/saved_model_gensim-gx2m_wjw\n",
            "2023-09-28 11:37:38,109 : INFO : loading FastText object from /tmp/saved_model_gensim-gx2m_wjw\n",
            "2023-09-28 11:37:38,128 : INFO : loading wv recursively from /tmp/saved_model_gensim-gx2m_wjw.wv.* with mmap=None\n",
            "2023-09-28 11:37:38,131 : INFO : loading vectors_ngrams from /tmp/saved_model_gensim-gx2m_wjw.wv.vectors_ngrams.npy with mmap=None\n",
            "2023-09-28 11:37:53,442 : INFO : setting ignored attribute buckets_word to None\n",
            "2023-09-28 11:37:53,444 : INFO : setting ignored attribute vectors to None\n",
            "2023-09-28 11:37:53,744 : INFO : setting ignored attribute cum_table to None\n",
            "2023-09-28 11:37:53,863 : INFO : FastText lifecycle event {'fname': '/tmp/saved_model_gensim-gx2m_wjw', 'datetime': '2023-09-28T11:37:53.863734', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'loaded'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<gensim.models.fasttext.FastText object at 0x7fac8beb42d0>\n"
          ]
        }
      ],
      "source": [
        "# Save a model trained via Gensim's fastText implementation to temp.\n",
        "import tempfile\n",
        "import os\n",
        "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
        "    model.save(tmp.name, separately=[])\n",
        "\n",
        "# Load back the same model.\n",
        "loaded_model = FastText.load(tmp.name)\n",
        "print(loaded_model)\n",
        "\n",
        "os.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``save_word2vec_format`` is also available for fastText models, but will\n",
        "cause all vectors for ngrams to be lost.\n",
        "As a result, a model loaded in this way will behave as a regular word2vec model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word vector lookup\n",
        "\n",
        "\n",
        "All information necessary for looking up fastText words (incl. OOV words) is\n",
        "contained in its ``model.wv`` attribute.\n",
        "\n",
        "If you don't need to continue training your model, you can export & save this `.wv`\n",
        "attribute and discard `model`, to save space and RAM.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<gensim.models.fasttext.FastTextKeyedVectors object at 0x7fac8bfb8b10>\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "wv = model.wv\n",
        "print(wv)\n",
        "\n",
        "#\n",
        "# FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n",
        "#\n",
        "print('night' in wv.key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print('nights' in wv.key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array([-0.12888013,  0.04867967, -0.24673456, -0.04103868,  0.05275234,\n",
            "        0.31245694,  0.4401131 ,  0.5945194 ,  0.20483033, -0.33483908,\n",
            "        0.08969966, -0.12318928, -0.3225757 ,  0.6259468 , -0.336348  ,\n",
            "       -0.5068071 ,  0.15760842, -0.18644254, -0.40220895, -0.5493166 ,\n",
            "       -0.4112685 ,  0.07552315, -0.5955141 , -0.09174105, -0.22124337,\n",
            "       -0.30470148, -0.55637515, -0.08589523, -0.18355782,  0.145315  ,\n",
            "       -0.28822622,  0.29130918,  0.83593965, -0.20667416,  0.19416878,\n",
            "        0.2799458 ,  0.5025871 , -0.02762571, -0.3422564 , -0.27625018,\n",
            "        0.5239989 , -0.4515748 ,  0.12707706, -0.25491595, -0.5585286 ,\n",
            "       -0.39250594,  0.07981683,  0.21418914,  0.19030538, -0.05546196,\n",
            "        0.33893386, -0.5403435 ,  0.30732256, -0.4247741 , -0.26652154,\n",
            "       -0.2702304 , -0.26582745, -0.08564702,  0.0222022 , -0.3513158 ,\n",
            "       -0.31648287, -0.40040338, -0.35052294,  0.37484387, -0.08553872,\n",
            "        0.7157797 ,  0.05957315,  0.05660595,  0.37558448,  0.37215015,\n",
            "       -0.2716772 ,  0.41949114,  0.57795066, -0.61145884,  0.28869936,\n",
            "        0.00513284,  0.24541527, -0.06345297,  0.04457372,  0.28457913,\n",
            "        0.16232362, -0.40856233, -0.75327575, -0.10711645, -0.11434623,\n",
            "       -0.6567547 ,  0.48177677,  0.25298864, -0.01482254, -0.32335693,\n",
            "       -0.08024716,  0.4411913 , -0.2443995 ,  0.12507556, -0.27158412,\n",
            "        0.5493198 , -0.1531908 , -0.20551334,  0.01927228, -0.23388354],\n",
            "      dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(wv['night'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array([-0.11204359,  0.04297431, -0.21327995, -0.0352643 ,  0.04430781,\n",
            "        0.26870054,  0.38157523,  0.5155037 ,  0.17746356, -0.290954  ,\n",
            "        0.07923076, -0.10493508, -0.2797879 ,  0.5387193 , -0.29198322,\n",
            "       -0.43849862,  0.13543466, -0.16058479, -0.34610537, -0.47520745,\n",
            "       -0.35236043,  0.06411079, -0.514322  , -0.08066469, -0.18978927,\n",
            "       -0.26179725, -0.4792625 , -0.07197589, -0.15831928,  0.12707758,\n",
            "       -0.24708374,  0.2511851 ,  0.72039205, -0.17814907,  0.16783813,\n",
            "        0.24126118,  0.43550962, -0.02385413, -0.29584044, -0.23919852,\n",
            "        0.45178562, -0.38916302,  0.10925046, -0.21967076, -0.4832682 ,\n",
            "       -0.33781338,  0.07183307,  0.18563451,  0.165774  , -0.04679913,\n",
            "        0.29431117, -0.4669891 ,  0.26610854, -0.3668553 , -0.22980894,\n",
            "       -0.23220178, -0.2315464 , -0.07219072,  0.02049286, -0.300834  ,\n",
            "       -0.27233016, -0.3461843 , -0.30240023,  0.32326448, -0.07330327,\n",
            "        0.6192912 ,  0.05157118,  0.04629685,  0.32418296,  0.3226109 ,\n",
            "       -0.23511916,  0.36042723,  0.5003299 , -0.5280076 ,  0.25087214,\n",
            "        0.00545345,  0.21136752, -0.05565856,  0.03825358,  0.24579859,\n",
            "        0.14069484, -0.35326043, -0.649903  , -0.09357426, -0.09774358,\n",
            "       -0.56834626,  0.4160263 ,  0.21867062, -0.01074945, -0.27990362,\n",
            "       -0.06925776,  0.37982845, -0.2115498 ,  0.10866241, -0.2350265 ,\n",
            "        0.474602  , -0.13366002, -0.17385638,  0.01699524, -0.20282814],\n",
            "      dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(wv['nights'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similarity operations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(\"nights\" in wv.key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(\"night\" in wv.key_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.99999166\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity(\"night\", \"nights\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Syntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. A detailed comparison is provided [here](Word2Vec_FastText_Comparison.ipynb).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other similarity operations\n",
        "\n",
        "The example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('night', 0.9999918341636658),\n",
            " ('rights', 0.9999877214431763),\n",
            " ('flights', 0.9999875426292419),\n",
            " ('overnight', 0.9999867677688599),\n",
            " ('fighting', 0.9999852180480957),\n",
            " ('fighters', 0.9999850392341614),\n",
            " ('fight', 0.9999849796295166),\n",
            " ('entered', 0.9999848008155823),\n",
            " ('fighter', 0.9999847412109375),\n",
            " ('eight', 0.9999843239784241)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(\"nights\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.99994105\n"
          ]
        }
      ],
      "source": [
        "print(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'lunch'\n"
          ]
        }
      ],
      "source": [
        "print(wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('capital,', 0.9996389746665955),\n",
            " ('find', 0.9996379017829895),\n",
            " ('findings', 0.999630868434906),\n",
            " ('field', 0.9996299147605896),\n",
            " ('finding', 0.9996282458305359),\n",
            " ('seekers.', 0.9996281862258911),\n",
            " ('abuse', 0.9996275305747986),\n",
            " ('had', 0.9996262192726135),\n",
            " ('storm', 0.9996260404586792),\n",
            " ('26-year-old', 0.9996227025985718)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 11:37:55,168 : INFO : Evaluating word analogies for top 300000 words in the model on /home/pat/VSCode/gensim/.venv/lib/python3.11/site-packages/gensim/test/test_data/questions-words.txt\n",
            "2023-09-28 11:37:55,280 : INFO : family: 0.0% (0/2)\n",
            "2023-09-28 11:37:55,359 : INFO : gram3-comparative: 8.3% (1/12)\n",
            "2023-09-28 11:37:55,386 : INFO : gram4-superlative: 33.3% (4/12)\n",
            "2023-09-28 11:37:55,446 : INFO : gram5-present-participle: 45.0% (9/20)\n",
            "2023-09-28 11:37:55,526 : INFO : gram6-nationality-adjective: 30.0% (6/20)\n",
            "2023-09-28 11:37:55,642 : INFO : gram7-past-tense: 5.0% (1/20)\n",
            "2023-09-28 11:37:55,675 : INFO : gram8-plural: 33.3% (4/12)\n",
            "2023-09-28 11:37:55,685 : INFO : Quadruplets with out-of-vocabulary words: 99.5%\n",
            "2023-09-28 11:37:55,688 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
            "2023-09-28 11:37:55,690 : INFO : Total accuracy: 25.5% (25/98)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.25510204081632654,\n",
            " [{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
            "  {'correct': [],\n",
            "   'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],\n",
            "   'section': 'family'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
            "  {'correct': [('LONG', 'LONGER', 'GREAT', 'GREATER')],\n",
            "   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
            "                 ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
            "                 ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
            "                 ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
            "                 ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
            "                 ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
            "                 ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
            "                 ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
            "                 ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
            "                 ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
            "                 ('LOW', 'LOWER', 'LONG', 'LONGER')],\n",
            "   'section': 'gram3-comparative'},\n",
            "  {'correct': [('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
            "               ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
            "               ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
            "               ('LARGE', 'LARGEST', 'BIG', 'BIGGEST')],\n",
            "   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
            "                 ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
            "                 ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
            "                 ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
            "                 ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
            "                 ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
            "                 ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
            "                 ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],\n",
            "   'section': 'gram4-superlative'},\n",
            "  {'correct': [('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
            "               ('GO', 'GOING', 'SAY', 'SAYING'),\n",
            "               ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
            "               ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
            "               ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
            "               ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
            "               ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
            "               ('SAY', 'SAYING', 'GO', 'GOING'),\n",
            "               ('SAY', 'SAYING', 'PLAY', 'PLAYING')],\n",
            "   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
            "                 ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
            "                 ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
            "                 ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
            "                 ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
            "                 ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
            "                 ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
            "                 ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
            "                 ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
            "                 ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
            "                 ('SAY', 'SAYING', 'RUN', 'RUNNING')],\n",
            "   'section': 'gram5-present-participle'},\n",
            "  {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
            "               ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
            "               ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
            "               ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
            "               ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "               ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN')],\n",
            "   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
            "                 ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
            "                 ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],\n",
            "   'section': 'gram6-nationality-adjective'},\n",
            "  {'correct': [('PAYING', 'PAID', 'SAYING', 'SAID')],\n",
            "   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
            "                 ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
            "                 ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
            "                 ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
            "                 ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
            "                 ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
            "                 ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
            "                 ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
            "                 ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
            "                 ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
            "                 ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
            "                 ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
            "                 ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
            "                 ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
            "                 ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
            "                 ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
            "                 ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
            "                 ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
            "                 ('TAKING', 'TOOK', 'SAYING', 'SAID')],\n",
            "   'section': 'gram7-past-tense'},\n",
            "  {'correct': [('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
            "               ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
            "               ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
            "               ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
            "   'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
            "                 ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
            "                 ('CAR', 'CARS', 'MAN', 'MEN'),\n",
            "                 ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
            "                 ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
            "                 ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
            "                 ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
            "                 ('MAN', 'MEN', 'CAR', 'CARS')],\n",
            "   'section': 'gram8-plural'},\n",
            "  {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
            "  {'correct': [('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
            "               ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
            "               ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
            "               ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
            "               ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
            "               ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
            "               ('GO', 'GOING', 'SAY', 'SAYING'),\n",
            "               ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
            "               ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
            "               ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
            "               ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
            "               ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
            "               ('SAY', 'SAYING', 'GO', 'GOING'),\n",
            "               ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
            "               ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
            "               ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
            "               ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
            "               ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
            "               ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "               ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
            "               ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
            "               ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
            "               ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
            "               ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
            "               ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
            "   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
            "                 ('HIS', 'HER', 'HE', 'SHE'),\n",
            "                 ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
            "                 ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
            "                 ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
            "                 ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
            "                 ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
            "                 ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
            "                 ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
            "                 ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
            "                 ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
            "                 ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
            "                 ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
            "                 ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
            "                 ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
            "                 ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
            "                 ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
            "                 ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
            "                 ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
            "                 ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
            "                 ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
            "                 ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
            "                 ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
            "                 ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
            "                 ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
            "                 ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
            "                 ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
            "                 ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
            "                 ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
            "                 ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
            "                 ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
            "                 ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
            "                 ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
            "                 ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
            "                 ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
            "                 ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
            "                 ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
            "                 ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
            "                 ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
            "                 ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
            "                 ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
            "                 ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
            "                 ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
            "                 ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
            "                 ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
            "                 ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
            "                 ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
            "                 ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
            "                 ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
            "                 ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
            "                 ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
            "                 ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
            "                 ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
            "                 ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
            "                 ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
            "                 ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
            "                 ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
            "                 ('CAR', 'CARS', 'MAN', 'MEN'),\n",
            "                 ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
            "                 ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
            "                 ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
            "                 ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
            "                 ('MAN', 'MEN', 'CAR', 'CARS')],\n",
            "   'section': 'Total accuracy'}])\n"
          ]
        }
      ],
      "source": [
        "print(wv.evaluate_word_analogies(datapath('questions-words.txt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Movers distance\n",
        "\n",
        "You'll need the optional ``POT`` library for this section, ``pip install POT``.\n",
        "\n",
        "Let's start with two sentences:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
        "sentence_president = 'The president greets the press in Chicago'.lower().split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove their stopwords.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "sentence_obama = [w for w in sentence_obama if w not in STOPWORDS]\n",
        "sentence_president = [w for w in sentence_president if w not in STOPWORDS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the Word Movers Distance between the two sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-28 11:37:56,240 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
            "2023-09-28 11:37:56,242 : INFO : built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\n",
            "2023-09-28 11:37:56,245 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\", 'datetime': '2023-09-28T11:37:56.245031', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jun  9 2023, 07:59:55) [GCC 12.3.0]', 'platform': 'Linux-6.2.0-33-generic-x86_64-with-glibc2.37', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Word Movers Distance is 0.016112898508040444 (lower means closer)'\n"
          ]
        }
      ],
      "source": [
        "distance = wv.wmdistance(sentence_obama, sentence_president)\n",
        "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's all! You've made it to the end of this tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fasttext-logo-color-web.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpimg\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m img \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39;49mimread(\u001b[39m'\u001b[39;49m\u001b[39mfasttext-logo-color-web.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m imgplot \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mimshow(img)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pat/VSCode/gensim/auto_examples/tutorials/run_fasttext.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m _ \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/VSCode/gensim/.venv/lib/python3.11/site-packages/matplotlib/image.py:1525\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1519\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1521\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         )\n\u001b[0;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m   1526\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1527\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m             pil_to_array(image))\n",
            "File \u001b[0;32m~/VSCode/gensim/.venv/lib/python3.11/site-packages/PIL/ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[1;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fasttext-logo-color-web.png'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread('fasttext-logo-color-web.png')\n",
        "imgplot = plt.imshow(img)\n",
        "_ = plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
